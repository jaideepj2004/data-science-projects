{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get the html page using the beautiful soup \n",
    "2. get role , company , duration,location , stipend and requirement \n",
    "3. saving all the details into a csv,excel,google docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: Machine Learning\n",
      "Company: Artificial Intelligence (AI)\n",
      "Role: Coding Content Creator\n",
      "Company: Research Analytics HR\n",
      "Role: Data Analytics\n",
      "Company: Business Analytics\n",
      "Role: Data Analytics\n",
      "Company: Content Creation - Full Stack and Data Science/AI\n",
      "Role: Business Analysis And Market Research\n",
      "Company: Business Analyst\n",
      "Role: ML Engineering\n",
      "Company: Computer Vision (Machine Learning)\n",
      "Role: Business Analytics\n",
      "Company: Technical Lead Generation & Business Prospects (IT)\n",
      "Role: Robotics/AI/ML & Embedded Systems\n",
      "Company: Data Analytics\n",
      "Role: MBA/Finance/HR/CA Operations\n",
      "Company: Full-stack Data Engineering\n",
      "Role: Business Analyst\n",
      "Company: Data Analytics\n",
      "Role: Data Analytics\n",
      "Company: Content Creation\n",
      "Role: Business Analytics\n",
      "Company: Business Analytics\n",
      "Role: Business Analyst\n",
      "Company: STEM Mentoring\n",
      "Role: Web Development & Google Analytics\n",
      "Company: Data Analysis\n",
      "Role: Content Analytics Coordination\n",
      "Company: Blockchain Research Analytics\n",
      "Role: Marketing And Market Analystics\n",
      "Company: Generative Adversarial Networks\n",
      "Role: Market Research Analytics\n",
      "Company: Sports Analytics\n",
      "Role: Market Analytics\n",
      "Company: Data Analysis\n",
      "Role: Business Research & Business Analytics\n",
      "Company: Data Analytics\n",
      "Role: Data Analysis & Leadership\n",
      "Company: Business Analytics\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Create a list to store the stripped text\n",
    "    extracted_text_list = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        text = h3.get_text().strip()\n",
    "        extracted_text_list.append(text)\n",
    "\n",
    "    # Print the extracted data with titles\n",
    "    for i, text in enumerate(extracted_text_list):\n",
    "        if i % 2 == 0:\n",
    "            title = \"Role\"\n",
    "        else:\n",
    "            title = \"Company\"\n",
    "        print(f\"{title}: {text}\")\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Create a list to store the stripped text\n",
    "    extracted_button_text_list = []\n",
    "\n",
    "    for element in button_elements:\n",
    "        button_text = element.get_text().strip()\n",
    "        extracted_button_text_list.append(button_text)\n",
    "\n",
    "    # Print the extracted button text with a heading\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company:\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Create a list to store the stripped text\n",
    "    extracted_text_list = []\n",
    "\n",
    "    for element in elements:\n",
    "        text = element.get_text().strip()\n",
    "        extracted_text_list.append(text)\n",
    "\n",
    "    # Print the extracted text with a heading \"Company\"\n",
    "    print(\"Company:\")\n",
    "    for text in extracted_text_list:\n",
    "        print(text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Create a list to store the stripped text\n",
    "    extracted_text_list = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_text_list.append(anchor.get_text().strip())\n",
    "\n",
    "    # Print the title \"Role\"\n",
    "    print(\"Role:\")\n",
    "\n",
    "    # Print the extracted text\n",
    "    for text in extracted_text_list:\n",
    "        print(text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Content Creation\n",
      "Full\n",
      "Company:\n",
      "Full Stack and Data Science/AI\n",
      "stack Data Engineering\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        text = h3.get_text().strip()\n",
    "        # Split the text into role and company based on a delimiter (assuming a hyphen as a separator)\n",
    "        parts = text.split('-')\n",
    "        if len(parts) == 2:\n",
    "            role, company = parts[0].strip(), parts[1].strip()\n",
    "            extracted_roles.append(role)\n",
    "            extracted_companies.append(company)\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Create a list to store the stripped location text\n",
    "    extracted_locations = []\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        location_text = location_element.get_text().strip()\n",
    "        extracted_locations.append(location_text)\n",
    "\n",
    "    # Print the extracted data\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Create a list to store the stripped text\n",
    "    extracted_button_text_list = []\n",
    "\n",
    "    for element in button_elements:\n",
    "        button_text = element.get_text().strip()\n",
    "        extracted_button_text_list.append(button_text)\n",
    "\n",
    "    # Print the extracted button text with a heading\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Company:\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all \"h3\" elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Company :\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    print(\"Company :\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 25,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 45,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 1,250-12,500 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 30,000-40,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 1,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 2,000-4,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 6,000-8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 12,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 16,000-21,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 2,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 16,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 2,000 /month +  Incentives\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 10,000 /month\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    duration_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "    extracted_durations = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for duration_element in duration_elements:\n",
    "        extracted_durations.append(duration_element.get_text().strip())\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "Duration:\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "    extracted_durations = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        text = item_body_element.get_text().strip()\n",
    "        # Check if the element contains \"Stipend\" or \"Duration\" and extract accordingly\n",
    "        if text.startswith(\"Stipend\"):\n",
    "            extracted_stipends.append(text)\n",
    "        elif text.startswith(\"Duration\"):\n",
    "            extracted_durations.append(text)\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 25,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 45,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 1,250-12,500 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 30,000-40,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 1,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 2,000-4,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 6,000-8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 12,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 16,000-21,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 2,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 16,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 2,000 /month +  Incentives\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 10,000 /month\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    duration_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "    extracted_durations = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for duration_element in duration_elements:\n",
    "        text = duration_element.get_text().strip()\n",
    "        # Check if the element contains \"Stipend\" and skip it\n",
    "        if not text.startswith(\"Stipend\"):\n",
    "            extracted_durations.append(text)\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "Duration:\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "    extracted_durations = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        text = item_body_element.get_text().strip()\n",
    "        # Check if the element contains \"Stipend\" or \"Duration\" and extract accordingly\n",
    "        if \"Stipend\" in text:\n",
    "            extracted_stipends.append(text)\n",
    "        elif \"Duration\" in text:\n",
    "            # Remove the \"Stipend\" part from the \"Duration\"\n",
    "            duration = text.replace(\"Stipend\", \"\").strip()\n",
    "            extracted_durations.append(duration)\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 25,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 45,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 1,250-12,500 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 30,000-40,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 1,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 2,000-4,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 10,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 6,000-8,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 5,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000-18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 6,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 12,000-15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 20,000 /month\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 15,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 16,000-21,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 2,000 /month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "₹ 8,000-12,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 18,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 16,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 14,000 /month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "₹ 2,000 /month +  Incentives\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 5,000-10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "₹ 10,000 /month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "₹ 10,000 /month\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "         extracted_durations.append(item_body_element.get_text().strip())\n",
    "        # Extract the text from the item_body\n",
    "       \n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    # Print all the text for duration\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        text = item_body_element.get_text().strip()\n",
    "        extracted_durations.append(text)\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    # Print durations excluding those containing '/month'\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        if '/month' not in duration:\n",
    "            print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://internshala.com//internship/detail/machine-learning-work-from-home-job-internship-at-livesitter1697178737\n",
      "https://internshala.com//internship/detail/artificial-intelligence-ai-internship-in-gurgaon-at-natwest-group1697112067\n",
      "https://internshala.com//internship/detail/coding-content-creator-work-from-home-job-internship-at-hudi-enterprise1697086342\n",
      "https://internshala.com//internship/detail/research-analytics-hr-work-from-home-job-internship-at-hr-inputs1697193637\n",
      "https://internshala.com//internship/detail/data-analytics-internship-in-mumbai-at-perusal-global-solutions1697192477\n",
      "https://internshala.com//internship/detail/business-analytics-internship-in-hyderabad-at-hilo-design1697182190\n",
      "https://internshala.com//internship/detail/data-analytics-internship-in-mumbai-at-beco1697185441\n",
      "https://internshala.com//internship/detail/content-creation-full-stack-and-data-science-ai-work-from-home-job-internship-at-banogeniuss1697177946\n",
      "https://internshala.com//internship/detail/business-analysis-and-market-research-work-from-home-job-internship-at-madee1697176734\n",
      "https://internshala.com//internship/detail/business-analyst-internship-in-noida-at-fellafeeds1697171995\n",
      "https://internshala.com//internship/detail/ml-engineering-work-from-home-job-internship-at-ranjana-enterprises1697170539\n",
      "https://internshala.com//internship/detail/computer-vision-machine-learning-work-from-home-job-internship-at-flickmatch1697133091\n",
      "https://internshala.com//internship/detail/business-analytics-internship-in-ahmedabad-at-yottol-thegoodnetwork-wealthtech1697046598\n",
      "https://internshala.com//internship/detail/technical-lead-generation-business-prospects-it-work-from-home-job-internship-at-rpa-infotech-private-limited1697014165\n",
      "https://internshala.com//internship/detail/robotics-ai-ml-embedded-systems-internship-in-delhi-at-logicboots-private-limited1696944903\n",
      "https://internshala.com//internship/detail/data-analytics-work-from-home-job-internship-at-indika-ai-private-limited1697105408\n",
      "https://internshala.com//internship/detail/mba-finance-hr-ca-operations-internship-in-delhi-at-srivatsa-cs-and-co1697095085\n",
      "https://internshala.com//internship/detail/full-stack-data-engineering-work-from-home-job-internship-at-blackcoffer1697033471\n",
      "https://internshala.com//internship/detail/business-analyst-work-from-home-job-internship-at-analysttai1697028000\n",
      "https://internshala.com//internship/detail/data-analytics-internship-in-north-at-sandeep-enterprises1697081113\n",
      "https://internshala.com//internship/detail/data-analytics-part-time-job-internship-at-bangalore-in-cry-child-rights-and-you1696910847\n",
      "https://internshala.com//internship/detail/content-creation-work-from-home-job-internship-at-learnwithlife1697030156\n",
      "https://internshala.com//internship/detail/business-analytics-internship-in-chennai-at-texcel-engineers-private-limited1697018271\n",
      "https://internshala.com//internship/detail/business-analytics-internship-in-multiple-locations-at-flying-fish-accessories1697023225\n",
      "https://internshala.com//internship/detail/business-analyst-internship-in-ahmedabad-at-bombay-softwares1696508781\n",
      "https://internshala.com//internship/detail/stem-mentoring-part-time-job-internship-at-mumbai-in-metx-robotics1697017588\n",
      "https://internshala.com//internship/detail/web-development-google-analytics-internship-in-mumbai-at-fruitilicious1697020502\n",
      "https://internshala.com//internship/detail/data-analysis-work-from-home-job-internship-at-gamahouse-publishing1697020129\n",
      "https://internshala.com//internship/detail/content-analytics-coordination-work-from-home-job-internship-at-gamahouse-publishing1697018626\n",
      "https://internshala.com//internship/detail/blockchain-research-analytics-work-from-home-job-internship-at-metaverse-ventures-private-limited1697008961\n",
      "https://internshala.com//internship/detail/marketing-and-market-analystics-internship-in-visakhapatnam-at-goprayaan1697016546\n",
      "https://internshala.com//internship/detail/generative-adversarial-networks-work-from-home-job-internship-at-across-the-globe-atg1697003765\n",
      "https://internshala.com//internship/detail/market-research-analytics-work-from-home-job-internship-at-gamahouse-publishing1697014340\n",
      "https://internshala.com//internship/detail/sports-analytics-work-from-home-job-internship-at-gamahouse-publishing1697013293\n",
      "https://internshala.com//internship/detail/market-analytics-work-from-home-job-internship-at-gamahouse-publishing1697010351\n",
      "https://internshala.com//internship/detail/data-analysis-work-from-home-job-internship-at-gamahouse-publishing1697010288\n",
      "https://internshala.com//internship/detail/business-research-business-analytics-internship-in-noida-at-orbiqe-technologies-private-limited1697008172\n",
      "https://internshala.com//internship/detail/data-analytics-internship-in-kolkata-at-aero-armour1697001102\n",
      "https://internshala.com//internship/detail/data-analysis-leadership-work-from-home-job-internship-at-gamahouse-publishing1697004623\n",
      "https://internshala.com//internship/detail/business-analytics-work-from-home-job-internship-at-gamahouse-publishing1697004336\n",
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "Coding Content Creator\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Data Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "ML Engineering\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Company:\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 1,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Text from elements with class 'link_display_like_text view_detail_button':\n",
      "Livesitter\n",
      "NatWest Group\n",
      "Hudi Enterprise\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "Beco\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "Ranjana Enterprises\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"btn btn-secondary view_detail_button_outline\"\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "\n",
    "    # Extract and print the links from these elements\n",
    "    for link_element in link_elements:\n",
    "        link = link_element.get(\"href\")\n",
    "        if link:\n",
    "            print(\"https://internshala.com/\" + link)\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "         extracted_durations.append(item_body_element.get_text().strip())\n",
    "        # Extract the text from the item_body\n",
    "       \n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    print(\"Company:\")\n",
    "    for company in extracted_companies:\n",
    "        print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    # Print all the text for duration\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        if '/month' not in duration:\n",
    "            print(duration)\n",
    "\n",
    "    # Print durations excluding those containing '/month'\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        if '/month' not in duration:\n",
    "            print(duration)\n",
    "\n",
    "    print(\"Text from elements with class 'link_display_like_text view_detail_button':\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening link: https://internshala.com//internship/detail/machine-learning-work-from-home-job-internship-at-livesitter1697178737\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 13th Oct'23 and 17th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/artificial-intelligence-ai-internship-in-gurgaon-at-natwest-group1697112067\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Gurgaon and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/coding-content-creator-work-from-home-job-internship-at-hudi-enterprise1697086342\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/research-analytics-hr-work-from-home-job-internship-at-hr-inputs1697193637\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-internship-in-mumbai-at-perusal-global-solutions1697192477\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Mumbai and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analytics-internship-in-hyderabad-at-hilo-design1697182190\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 13th Oct'23 and 17th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-internship-in-mumbai-at-beco1697185441\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/content-creation-full-stack-and-data-science-ai-work-from-home-job-internship-at-banogeniuss1697177946\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 2 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analysis-and-market-research-work-from-home-job-internship-at-madee1697176734\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analyst-internship-in-noida-at-fellafeeds1697171995\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Noida and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/ml-engineering-work-from-home-job-internship-at-ranjana-enterprises1697170539\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/computer-vision-machine-learning-work-from-home-job-internship-at-flickmatch1697133091\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analytics-internship-in-ahmedabad-at-yottol-thegoodnetwork-wealthtech1697046598\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Ahmedabad                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/technical-lead-generation-business-prospects-it-work-from-home-job-internship-at-rpa-infotech-private-limited1697014165\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/robotics-ai-ml-embedded-systems-internship-in-delhi-at-logicboots-private-limited1696944903\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Delhi and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-work-from-home-job-internship-at-indika-ai-private-limited1697105408\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/mba-finance-hr-ca-operations-internship-in-delhi-at-srivatsa-cs-and-co1697095085\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/full-stack-data-engineering-work-from-home-job-internship-at-blackcoffer1697033471\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analyst-work-from-home-job-internship-at-analysttai1697028000\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-internship-in-north-at-sandeep-enterprises1697081113\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to North                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-part-time-job-internship-at-bangalore-in-cry-child-rights-and-you1696910847\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the part time job/internship                    \n",
      "\n",
      "                        2. can start the part time job/internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/content-creation-work-from-home-job-internship-at-learnwithlife1697030156\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analytics-internship-in-chennai-at-texcel-engineers-private-limited1697018271\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analytics-internship-in-multiple-locations-at-flying-fish-accessories1697023225\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analyst-internship-in-ahmedabad-at-bombay-softwares1696508781\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/stem-mentoring-part-time-job-internship-at-mumbai-in-metx-robotics1697017588\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the part time job/internship                    \n",
      "\n",
      "                        2. can start the part time job/internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Mumbai and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/web-development-google-analytics-internship-in-mumbai-at-fruitilicious1697020502\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analysis-work-from-home-job-internship-at-gamahouse-publishing1697020129\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/content-analytics-coordination-work-from-home-job-internship-at-gamahouse-publishing1697018626\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/blockchain-research-analytics-work-from-home-job-internship-at-metaverse-ventures-private-limited1697008961\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/marketing-and-market-analystics-internship-in-visakhapatnam-at-goprayaan1697016546\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/generative-adversarial-networks-work-from-home-job-internship-at-across-the-globe-atg1697003765\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/market-research-analytics-work-from-home-job-internship-at-gamahouse-publishing1697014340\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/sports-analytics-work-from-home-job-internship-at-gamahouse-publishing1697013293\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/market-analytics-work-from-home-job-internship-at-gamahouse-publishing1697010351\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analysis-work-from-home-job-internship-at-gamahouse-publishing1697010288\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-research-business-analytics-internship-in-noida-at-orbiqe-technologies-private-limited1697008172\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 2 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analytics-internship-in-kolkata-at-aero-armour1697001102\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Kolkata                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/data-analysis-leadership-work-from-home-job-internship-at-gamahouse-publishing1697004623\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Opening link: https://internshala.com//internship/detail/business-analytics-work-from-home-job-internship-at-gamahouse-publishing1697004336\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"btn btn-secondary view_detail_button_outline\"\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "\n",
    "    # Create a list to store links\n",
    "    links = []\n",
    "\n",
    "    # Extract and store the links\n",
    "    for link_element in link_elements:\n",
    "        link = link_element.get(\"href\")\n",
    "        if link:\n",
    "            links.append(\"https://internshala.com/\" + link)\n",
    "\n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        print(\"Opening link:\", link)\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            inner_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            who_can_apply_elements = inner_soup.find_all(class_=\"text-container who_can_apply\")\n",
    "            for who_can_apply_element in who_can_apply_elements:\n",
    "                text = who_can_apply_element.get_text()\n",
    "                print(\"Text from 'who_can_apply':\", text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "Machine Learning\n",
      "Artificial Intelligence (AI)\n",
      "HR Analytics\n",
      "Product Analytics\n",
      "Research Analytics HR\n",
      "Data Analytics\n",
      "Business Analytics\n",
      "Content Creation - Full Stack and Data Science/AI\n",
      "Business Analysis And Market Research\n",
      "Business Analyst\n",
      "Computer Vision (Machine Learning)\n",
      "Business Analytics\n",
      "Technical Lead Generation & Business Prospects (IT)\n",
      "Robotics/AI/ML & Embedded Systems\n",
      "Coding Content Creator\n",
      "Data Analytics\n",
      "MBA/Finance/HR/CA Operations\n",
      "Full-stack Data Engineering\n",
      "Business Analyst\n",
      "Data Analytics\n",
      "Data Analytics\n",
      "Content Creation\n",
      "Business Analytics\n",
      "Business Analytics\n",
      "Business Analyst\n",
      "STEM Mentoring\n",
      "Web Development & Google Analytics\n",
      "Data Analysis\n",
      "Content Analytics Coordination\n",
      "Blockchain Research Analytics\n",
      "Marketing And Market Analystics\n",
      "Generative Adversarial Networks\n",
      "Market Research Analytics\n",
      "Sports Analytics\n",
      "Market Analytics\n",
      "Data Analysis\n",
      "Business Research & Business Analytics\n",
      "Data Analytics\n",
      "Data Analysis & Leadership\n",
      "Business Analytics\n",
      "Location:\n",
      "Work From Home\n",
      "Gurgaon\n",
      "Work From Home\n",
      "Bangalore\n",
      "Work From Home\n",
      "Mumbai\n",
      "Hyderabad\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Work From Home\n",
      "Ahmedabad\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "Delhi\n",
      "Work From Home\n",
      "Work From Home\n",
      "North\n",
      "Bangalore\n",
      "Work From Home\n",
      "Chennai\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Thane\n",
      "Ahmedabad\n",
      "Mumbai\n",
      "Mumbai\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Visakhapatnam\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Work From Home\n",
      "Noida\n",
      "Kolkata\n",
      "Work From Home\n",
      "Work From Home\n",
      "Stipend:\n",
      "₹ 25,000 /month\n",
      "₹ 45,000 /month\n",
      "₹ 30,000 /month\n",
      "₹ 25,000 /month\n",
      "₹ 6,000-12,000 /month\n",
      "₹ 15,000-20,000 /month\n",
      "₹ 8,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 1,250-12,500 /month\n",
      "₹ 30,000-40,000 /month\n",
      "₹ 2,000-4,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 10,000-15,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 6,000-8,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 5,000 /month\n",
      "₹ 15,000-18,000 /month\n",
      "Unpaid\n",
      "₹ 14,000 /month\n",
      "₹ 6,000 /month\n",
      "₹ 12,000-15,000 /month\n",
      "₹ 20,000 /month\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 15,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 16,000-21,000 /month\n",
      "₹ 2,000 /month\n",
      "₹ 8,000-12,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 18,000 /month\n",
      "₹ 16,000 /month\n",
      "₹ 14,000 /month\n",
      "₹ 2,000 /month +  Incentives\n",
      "₹ 5,000-10,000 /month\n",
      "₹ 10,000 /month\n",
      "₹ 10,000 /month\n",
      "Duration:\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Unpaid\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "3 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "6 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "2 Months\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Starts immediatelyImmediately\n",
      "1 Month\n",
      "Starts immediatelyImmediately\n",
      "4 Months\n",
      "Company\n",
      "Livesitter\n",
      "NatWest Group\n",
      "GoKwik Commerce Solutions Private Limited\n",
      "AerX Labs India Private Limited\n",
      "HR INPUTS\n",
      "Perusal Global Solutions\n",
      "Hilo Design\n",
      "BanoGeniuss\n",
      "Madee\n",
      "FellaFeeds\n",
      "FlickMatch\n",
      "Yottol-TheGoodNetwork (WealthTech)\n",
      "RPA Infotech Private Limited\n",
      "Logicboots Private Limited\n",
      "Hudi Enterprise\n",
      "Indika AI Private Limited\n",
      "Srivatsa CS And Co.\n",
      "Blackcoffer\n",
      "Analystt.ai\n",
      "Sandeep Enterprises\n",
      "CRY (Child Rights And You)\n",
      "Learnwithlife\n",
      "Texcel Engineers Private Limited\n",
      "Flying Fish Accessories\n",
      "Bombay Softwares\n",
      "Metx Robotics\n",
      "Fruitilicious\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Metaverse Ventures Private Limited\n",
      "GoPrayaan\n",
      "Across The Globe (ATG)\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Orbiqe Technologies Private Limited\n",
      "Aero Armour\n",
      "Gamahouse Publishing\n",
      "Gamahouse Publishing\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 13th Oct'23 and 17th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Gurgaon and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Bangalore                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Mumbai and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 13th Oct'23 and 17th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 2 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Noida and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Ahmedabad                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Delhi and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 12th Oct'23 and 16th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to North                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the part time job/internship                    \n",
      "\n",
      "                        2. can start the part time job/internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the part time job/internship                    \n",
      "\n",
      "                        2. can start the part time job/internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 3 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Mumbai and neighboring cities                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 10th Oct'23 and 14th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 6 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 9th Oct'23 and 13th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 2 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for full time (in-office) internship                    \n",
      "\n",
      "                        2. can start the internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4.  are from or open to relocate to Kolkata                    \n",
      "\n",
      "                        5. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 1 month                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "* Women wanting to start/restart their career can also apply.\n",
      "\n",
      "Text from 'who_can_apply': \n",
      "Only those candidates can apply who:\n",
      "\n",
      "                        1. are available for the work from home job/internship                    \n",
      "\n",
      "                        2. can start the work from home job/internship between 11th Oct'23 and 15th Nov'23                    \n",
      "\n",
      "                        3. are available for duration of 4 months                    \n",
      "\n",
      "                        4. have relevant skills and interests                    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"btn btn-secondary view_detail_button_outline\"\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "\n",
    "    # Create a list to store links\n",
    "    links = []\n",
    "\n",
    "    # Extract and store the links\n",
    "    for link_element in link_elements:\n",
    "        link = link_element.get(\"href\")\n",
    "        if link:\n",
    "            links.append(\"https://internshala.com/\" + link)\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        extracted_durations.append(item_body_element.get_text().strip())\n",
    "        # Extract the text from the item_body\n",
    "\n",
    "    # Print the extracted data with headings\n",
    "    print(\"Role:\")\n",
    "    for role in extracted_roles:\n",
    "        print(role)\n",
    "\n",
    "    # print(\"Company:\")\n",
    "    # for company in extracted_companies:\n",
    "    #     print(company)\n",
    "\n",
    "    print(\"Location:\")\n",
    "    for location in extracted_locations:\n",
    "        print(location)\n",
    "\n",
    "    print(\"Stipend:\")\n",
    "    for stipend in extracted_stipends:\n",
    "        print(stipend)\n",
    "\n",
    "    # Print all the text for duration\n",
    "    print(\"Duration:\")\n",
    "    for duration in extracted_durations:\n",
    "        if '/month' not in duration:\n",
    "            print(duration)\n",
    "    \n",
    "    print(\"Company\")\n",
    "    for button_text in extracted_button_text_list:\n",
    "        print(button_text)\n",
    "\n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        # print(\"Opening link:\", link)\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            inner_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            who_can_apply_elements = inner_soup.find_all(class_=\"text-container who_can_apply\")\n",
    "            for who_can_apply_element in who_can_apply_elements:\n",
    "                text = who_can_apply_element.get_text()\n",
    "                print(\"Text from 'who_can_apply':\", text)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jaide\\OneDrive\\Desktop\\Data Science\\python\\freelancing project\\internshala\\dataScience_scrapping.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaide/OneDrive/Desktop/Data%20Science/python/freelancing%20project/internshala/dataScience_scrapping.ipynb#X30sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Process each link\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaide/OneDrive/Desktop/Data%20Science/python/freelancing%20project/internshala/dataScience_scrapping.ipynb#X30sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jaide/OneDrive/Desktop/Data%20Science/python/freelancing%20project/internshala/dataScience_scrapping.ipynb#X30sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(link)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaide/OneDrive/Desktop/Data%20Science/python/freelancing%20project/internshala/dataScience_scrapping.ipynb#X30sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaide/OneDrive/Desktop/Data%20Science/python/freelancing%20project/internshala/dataScience_scrapping.ipynb#X30sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m         inner_soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[0;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[0;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1076\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1075\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1076\u001b[0m chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_chunk(amt)\n\u001b[0;32m   1077\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   1078\u001b[0m     chunk, decode_content\u001b[39m=\u001b[39mdecode_content, flush_decoder\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m )\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m decoded:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1018\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left:\n\u001b[1;32m-> 1018\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49m_safe_read(amt)  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m-\u001b[39m amt\n\u001b[0;32m   1020\u001b[0m     returned_chunk \u001b[39m=\u001b[39m value\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[0;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[0;32m    633\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"btn btn-secondary view_detail_button_outline\"\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "\n",
    "    # Create a list to store links\n",
    "    links = []\n",
    "\n",
    "    # Extract and store the links\n",
    "    for link_element in link_elements:\n",
    "        link = link_element.get(\"href\")\n",
    "        if link:\n",
    "            links.append(\"https://internshala.com/\" + link)\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_companies = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        extracted_durations.append(item_body_element.get_text().strip())\n",
    "        # Extract the text from the item_body\n",
    "\n",
    "    company_index = 0  # Track the company index while scraping who_can_apply\n",
    "\n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            inner_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            who_can_apply_elements = inner_soup.find_all(class_=\"text-container who_can_apply\")\n",
    "            for who_can_apply_element in who_can_apply_elements:\n",
    "                text = who_can_apply_element.get_text()\n",
    "                extracted_companies.append(extracted_roles[company_index])  # Append the company for the role\n",
    "                company_index += 1  # Increment the company index\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "\n",
    "    # Prepare the scraped data as a list of tuples\n",
    "    csv_data = list(zip(extracted_roles, extracted_companies, extracted_locations, extracted_stipends, extracted_durations, extracted_button_text_list))\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(\"internship_data.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"Role\", \"Company\", \"Location\", \"Stipend\", \"Duration\", \"Button Text\"])\n",
    "        csv_writer.writerows(csv_data)\n",
    "\n",
    "    print(\"Data has been saved to internship_data.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get the html page using the beautiful soup \n",
    "2. get role , company , duration,location , stipend and requirement \n",
    "3. saving all the details into a csv,excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to internship_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with the class \"btn btn-secondary view_detail_button_outline\"\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "\n",
    "    # Create a list to store links\n",
    "    links = []\n",
    "\n",
    "    # Extract and store the links\n",
    "    for link_element in link_elements:\n",
    "        link = link_element.get(\"href\")\n",
    "        if link:\n",
    "            links.append(\"https://internshala.com/\" + link)\n",
    "\n",
    "    # Find all h3 elements\n",
    "    h3_elements = soup.find_all('h3')\n",
    "\n",
    "    # Find all elements with the class \"link_display_like_text view_detail_button\"\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"location_link view_detail_button\"\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "\n",
    "    # Find all elements with the class \"stipend\"\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "\n",
    "    # Find all elements with the class \"item_body\"\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "\n",
    "    # Create lists to store the stripped text\n",
    "    extracted_roles = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []  # To store stipend\n",
    "    extracted_durations = []  # To store duration\n",
    "    extracted_requirements = []  # To store requirements\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        # Find the anchor tag within the h3 element\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        extracted_durations.append(item_body_element.get_text().strip())\n",
    "        # Extract the text from the item_body\n",
    "\n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            inner_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            who_can_apply_elements = inner_soup.find_all(class_=\"text-container who_can_apply\")\n",
    "            for who_can_apply_element in who_can_apply_elements:\n",
    "                text = who_can_apply_element.get_text()\n",
    "                extracted_requirements.append(text)  # Append the requirements\n",
    "\n",
    "            # Find the number of months, if available\n",
    "            duration_match = re.search(r'\\d+ months?', response.text)\n",
    "            if duration_match:\n",
    "                extracted_durations[-1] = duration_match.group(0)\n",
    "\n",
    "    # Close the HTTP connection\n",
    "    response.close()\n",
    "\n",
    "    # Prepare the scraped data as a list of tuples\n",
    "    csv_data = list(zip(extracted_roles, extracted_locations, extracted_stipends, extracted_durations, extracted_button_text_list, extracted_requirements))\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(\"internship_data.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"Role\", \"Location\", \"Stipend\", \"Duration\", \"Button Text\", \"Requirement\"])\n",
    "        csv_writer.writerows(csv_data)\n",
    "\n",
    "    print(\"Data has been saved to internship_data.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to test_.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://internshala.com/internships/keywords-data%20science/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    link_elements = soup.find_all(class_=\"btn btn-secondary view_detail_button_outline\")\n",
    "    links = [\"https://internshala.com/\" + link.get(\"href\") for link in link_elements]\n",
    "\n",
    "    h3_elements = soup.find_all('h3')\n",
    "    button_elements = soup.find_all(class_=\"link_display_like_text view_detail_button\")\n",
    "    location_elements = soup.find_all(class_=\"location_link view_detail_button\")\n",
    "    stipend_elements = soup.find_all(class_=\"stipend\")\n",
    "    item_body_elements = soup.find_all(class_=\"item_body\")\n",
    "    \n",
    "    extracted_roles = []\n",
    "    extracted_locations = []\n",
    "    extracted_button_text_list = []\n",
    "    extracted_stipends = []\n",
    "    extracted_durations = []\n",
    "    extracted_requirements = []\n",
    "\n",
    "    for h3 in h3_elements:\n",
    "        anchor = h3.find('a')\n",
    "        if anchor:\n",
    "            extracted_roles.append(anchor.get_text().strip())\n",
    "\n",
    "    for element in button_elements:\n",
    "        extracted_button_text_list.append(element.get_text().strip())\n",
    "\n",
    "    for location_element in location_elements:\n",
    "        extracted_locations.append(location_element.get_text().strip())\n",
    "\n",
    "    for stipend_element in stipend_elements:\n",
    "        extracted_stipends.append(stipend_element.get_text().strip())\n",
    "\n",
    "    for item_body_element in item_body_elements:\n",
    "        text = item_body_element.get_text().strip()\n",
    "        if \"/month\" in text:\n",
    "            extracted_durations.append(text)\n",
    "        else:\n",
    "            extracted_durations.append(None)\n",
    "\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            inner_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            who_can_apply_elements = inner_soup.find_all(class_=\"text-container who_can_apply\")\n",
    "            for who_can_apply_element in who_can_apply_elements:\n",
    "                text = who_can_apply_element.get_text()\n",
    "                extracted_requirements.append(text)\n",
    "\n",
    "    # Ensure that all lists have the same length\n",
    "    min_length = min(len(extracted_roles), len(extracted_locations), len(extracted_button_text_list), len(extracted_stipends), len(extracted_durations), len(extracted_requirements))\n",
    "\n",
    "    # Prepare the scraped data as a DataFrame\n",
    "    data = {\n",
    "        \"Role\": extracted_roles[:min_length],\n",
    "        \"Location\": extracted_locations[:min_length],\n",
    "        \"Stipend\": extracted_stipends[:min_length],\n",
    "        \"Duration\": extracted_durations[:min_length],\n",
    "        \"Button Text\": extracted_button_text_list[:min_length],\n",
    "        \"Requirement\": extracted_requirements[:min_length]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove rows with None in the Duration column\n",
    "    df = df[df[\"Duration\"].notna()]\n",
    "\n",
    "    # Save the data to an Excel file\n",
    "    df.to_excel(\"test_.xlsx\", index=False)\n",
    "\n",
    "    print(\"Data has been saved to test_.xlsx\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
